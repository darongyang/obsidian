## 1.实验内容

**研究问题**。使用5种典型的优化方法（拉格朗日对偶法、梯度下降法、最速下降法、牛顿法和坐标下降法 ）来优化稀疏子集选择问题。
**问题的难点**。在实际场景中，稀疏性越大，准确性可能下降，需要实现多目标优化来平衡误差的最小化和稀疏性最大化。
**问题定义**。给定一个高维数据集  $X\in R^{n\times d}$  和目标值  $y\in R^{n}$  ，其中n是样本数，d是特征数。我们的目标是选择一个稀疏的特征子集S，并使用该子集来构建一个线性回归模型，最小化模型的预测误差和最大化稀疏性，公式如下:

$$\min_{\beta}\left[f_{1}(\beta)=\frac{1}{2n}\|X\beta-y\|_{2}^{2},\quad f_{2}(\beta)=\|\beta\|_{0}\right]\qquad(1)$$

其中:
- $\beta$是待优化的回归系数,大小为  $d\times 1$  ;
-  $f_1(\beta)$  表示均方误差(MSE),用于衡量模型的预测误差;
-  $f_{2}(\beta)$  表示零范数(即系数向量中非零元素的数量),用于衡量模型的稀疏性。
-  $\|\beta\|_{0}\leq k$  ,即非零特征数量不得超过阈值k;
## 2.实验方法
### 2.1 拉格朗日法

**拉格朗日函数**。 将上述问题构建拉格朗日函数如下：
$$
L(\beta,
v) = \frac{1}{2n} \|X\beta - y\|_2^2 +
v(\|\beta\|_0 - k)
$$
其中，$v$为对偶变量，$k$为非0特征数量（自行设置）。拉格朗日函数的对应代码为：
```python
def lagrange_func(X, y, beta, v, k):
    return get_mse(X, y, beta) + v * (get_sparsity_l0(beta) - k)
```

**拉格朗日法**。拉格朗日法通过对偶问题优化，计算原问题的最优解。但是，由于0范数无法求导和非凸性使得求解较为复杂。按照如下思路进行：（1）在固定$v$的情况下最小化$\beta$。 （2）固定$\beta$来最大化$v$，调整稀疏性约束。（3）交替进行这两个步骤，直到达到收敛。代码实现如下：
```python
def lagrange_dual(X, y, factor, k, iters=100000, tol=1e-4):
    n_samples, n_features = X.shape
    beta_new = np.zeros(n_features)
    v = 0.0001
    for iter in range(iters):
        beta_old = beta_new.copy()
        # 固定v优化beta
        beta_new = (sopt.minimize(beta_func, x0=beta_old, args=(X, y, v, k), method='SLSQP')).x
        # 固定beta优化v
        v = sopt.minimize(v_func, x0=v, args=(beta_new, X, y, k), method='L-BFGS-B', bounds=[(0.00001, 0.0001)]).x
        if np.linalg.norm(beta_new - beta_old) < tol :
            break
    return beta_new
```

### 2.2 梯度下降法

**正则化函数**。由于之前的函数存在L0范数，不可导且非凸。需要将上述函数进行L1和L2正则化，得到一个无约束的凸优化模型，将零范数 $\| \beta \|_0$ 替换为平滑的正则化形式，新的优化函数定义如下：

$$
\min_{\beta} \left( f_1(\beta) = \frac{1}{2n} \| X\beta - y \|_2^2 + \lambda_1 \| \beta \|_1 + \lambda_2 \| \beta \|_2^2 \right)
\qquad (2)
$$
其中：
- $X$ 是设计矩阵
- $y$ 是观测值向量
- $\beta$ 是待求解的参数向量
- $\lambda_1$ 和 $\lambda_2$ 是正则化参数
后续的优化算法均基于此优化函数进行。
对应代码为：
```python
def normal_func(beta, X, y, lambda1, lambda2):
    mse = get_mse(X,y,beta)
    l1_norm = lambda1 * np.linalg.norm(beta, ord=1)
    l2_norm = lambda2 * np.linalg.norm(beta, ord=2)**2
    return mse + l1_norm + l2_norm
```

**函数梯度**。计算该函数的梯度函数 $\nabla f_1(\beta)$ ，L1不可微使用次梯度，最终的梯度表示为：
$$
\nabla  f_1(\beta) = \frac{1}{n} X^T (X\beta - y) + \lambda_1 \text{sign}(\beta) + 2\lambda_2\beta

$$
对应代码为：
```python
def get_gradient(beta, X, y, lambda1, lambda2):
    n_samples, _ = X.shape
    l1_subgrad = np.sign(beta) 
    l1_subgrad[l1_subgrad == 0] = np.random.choice([-1, 1], np.sum(l1_subgrad == 0))
    l2_grad = 2 *  beta
    mse_grad = X.T @ (X @ beta - y)
    return (1/n_samples) * mse_grad + lambda1 *  l1_subgrad + lambda2 * l2_grad
```

**梯度下降法**。按照梯度下降法的规则更新迭代参数，直到函数值收敛：
$$\beta_{k+1} = \beta_k - \eta \nabla f(\beta_k)$$
对应代码为：
```python
def gradient_descent(X, y, lambda1, lambda2, factor, iters=100000, tol=1e-4):
    _, n_features = X.shape
    beta_new = np.zeros(n_features)
    for iter in range(iters):
        beta_old = beta_new.copy()
        grad = get_gradient(beta_old, X, y, lambda1, lambda2)
        beta_new = beta_old - factor * grad
        if np.linalg.norm(beta_new - beta_old) < tol :
	        break
	return beta_new
```

### 2.3 最速下降法

**写在前面**。最速下降法，本来是指选取最优的下降方向。在本次实验指导书中，将最速下降法误写成了最优步长选取。这里遵循实验指导书，选择最优步长。但真正的最速下降法不应如此。

**最佳步长**。将步长step作为优化变量，在给定$\beta_{k}$的情况下，寻找使得函数值$\nabla f_1(\beta_k - step * \nabla f(\beta_k))$ 最小的step。对应代码如下：
```python
def step_func(step, beta,  X, y, lambda1, lambda2):
    return normal_func(beta - step * get_gradient(beta, X, y, lambda1, lambda2), X, y, lambda1, lambda2)

def search_best_step(beta, X, y, lambda1, lambda2, step):
    return (sopt.minimize(step_func, x0=0.5, args=(beta, X, y, lambda1, lambda2), method='BFGS')).x
```

**最速下降法**。在上述梯度下降法的基础上，加入对步长的最优选择，其他代码逻辑和梯度下降法一致。代码实现如下：
```python
def steepest_descent(X, y, lambda1, lambda2, step, iters=100000, tol=1e-4):
	...
    for iter in range(iters):
        ...
        step = search_best_step(beta_old, X, y, lambda1, lambda2, step)
        ...
    ...
```

### 2.4 Newton法

**牛顿法**。牛顿法通过二阶的Hessian矩阵优化了下降的方向，其更新规则变为：
$$\beta_{k+1} = \beta_k - \nabla ^2 f(\beta_k) \nabla f(\beta_k)$$
只需要将上述梯度下降法的下降方向更新为牛顿方向即可，实现代码如下：
```python
def get_hessian(X, lambda2):
    n_samples, n_features = X.shape
    return 2 * lambda2 * np.diag(np.ones(n_features)) + (1/n_samples) * X.T @ X
    
def newtown(X, y, lambda1, lambda2, iters=100000, tol=1e-4):
	...
	beta_new = beta_old - np.linalg.inv(get_hessian(X, lambda2)) @ grad
	...
```

### 2.5  坐标下降法

**坐标下降法**。坐标下降法在对优化变量 $\beta$ 进行迭代时，依次对优化变量的每个分量 $\beta_{j}$  进行优化更新，将所有分量 $\beta_{j}$ 都更新完成后视为完成一次对优化变量 $\beta$ 的迭代。其中优化某个分量 $\beta_{j}$  时，固定其他优化变量不变，更新规则如下：
$$\beta_j^{k+1} = \arg \min_{\beta_j} f(\beta_1^k, \ldots, \beta_{j-1}^k, \beta_j, \beta_{j+1}^k, \ldots, \beta_d^k)$$
对应实现代码如下：
```python
def beta_func_split(beta_j, j, beta_in, X, y, lambda1, lambda2):
    beta = beta_in.copy()
    beta[j] = beta_j
    return normal_func(beta, X, y, lambda1, lambda2)

def coordinate_descent(X, y, lambda1, lambda2, iters=100000, tol=1e-4):
    _, n_features = X.shape
    beta_new = np.ones(n_features)
    for iter in range(iters):
        beta_old = beta_new.copy()
        for j in range(n_features):
            beta_new[j] = (sopt.minimize(beta_func_split, x0=beta_old[j], args=(j, beta_old, X, y, lambda1, lambda2), method='BFGS')).x
        if np.linalg.norm(beta_new - beta_old) < tol:
            break
    return beta_new
```
## 3.实验结果和分析

### 3.1 拉格朗日法

**（1）改变正则化参数**
固定：样本数量为300，样本特征为50，稀疏性k为10。
改变：L1、L2正则化参数。

| L1、L2正则化参数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L0范数） | 执行时间(ms) |
| ---------- | ---- | --------- | --------- | -------- |
| 1e-5,1e-5  | 3    | 0.0000    | 50        | 366.22   |
| 1e-4,1e-4  | 3    | 0.0002    | 50        | 716.82   |
| 1e-3,1e-3  | 3    | 0.0002    | 50        | 287.37   |
从结果可以看出：
- 由于拉格朗日法总是求得近似解，不会全为0，L0范数无法准确得出稀疏性。
- 不同的正则化参数下，拉格朗日法的迭代次数总为3。原因是固定$\beta$优化v的时候，是关于v的线性函数，其最优解总在给定的边界值取到。因此只能迭代若干次。
### 3.2 梯度下降法
对所实现的梯度下降法进行若干组超参数测试。
**（1）改变正则化参数**
固定：样本数量为500，样本特征为100，学习率为0.01。
改变：L1、L2正则化参数。

| L1、L2正则化参数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ---------- | ---- | --------- | --------- | -------- |
| 1e-5,1e-5  | 1050 | 0.0002    | 9.182780  | 593.99   |
| 1e-4,1e-4  | 1049 | 0.0002    | 9.169973  | 396.11   |
| 1e-3,1e-3  | 1088 | 0.0002    | 9.051338  | 454.89   |
从结果可以看出：
- L1正则化参数越大，稀疏性越好。

**（2）改变学习率**
固定：样本数量为500，样本特征为100，L1和L2正则化参数为1e-4,1e-4。
改变：学习率。

| 学习率    | 迭代次数  | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ------ | ----- | --------- | --------- | -------- |
| 0.0001 | 11084 | 0.6316    | 9.555493  | 3504.77  |
| 0.001  | 4764  | 0.0116    | 9.976058  | 955.37   |
| 0.01   | 1049  | 0.0002    | 9.169973  | 318.47   |
从结果可以看出：
- 学习率较大，迭代次数少，更快迭代到最优解。所得到的最优解结果较为准确。
- 学习率过小，梯度下降法看起来效果并不好，迭代次数长，最优解结果不太合理

**（3）改变样本规模**
固定：L1和L2正则化参数为1e-4,1e-4，学习率为0.01。
改变：样本规模。

| 样本数，特征数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ------- | ---- | --------- | --------- | -------- |
| 100,20  | 919  | 0.0002    | 4.471946  | 84.32    |
| 300,50  | 955  | 0.0002    | 10.058503 | 328.89   |
| 500,100 | 1049 | 0.0002    | 9.169973  | 318.47   |
从结果可以看出：
- 样本规模越大，执行时间和迭代次数越久
- 在本实验结果中，小样本和大样本的稀疏性都比中等样本规模好

### 3.3 最速下降法

对所实现的最速下降法进行若干组超参数测试。本次实现所实现的最速下降法，选择最优步长，没有学习率的概念。

**（1）改变L1正则化参数**

固定：样本数量为300，样本特征为50。
改变：L1、L2正则化参数。

| L1、L2正则化参数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ---------- | ---- | --------- | --------- | -------- |
| 1e-5,1e-5  | 25   | 0.0000    | 10.013544 | 268.63   |
| 1e-4,1e-4  | -    | -         | -         | -        |
| 1e-3,1e-3  | 23   | 0.0001    | 9.967982  | 400.91   |
从结果可以看出：
- 当L1和L2取到1e-4,1e-4时，最速下降法没有收敛。
- 在1e-5,1e-5和1e-3,1e-3时，最速下降法结果收敛。

**（2）改变样本规模**

固定：L1和L2正则化参数为1e-5,1e-5。
改变：样本规模。

| 样本数，特征数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ------- | ---- | --------- | --------- | -------- |
| 100,20  | 29   | 0.0002    | 4.467682  | 93.85    |
| 300,50  | 25   | 0.0000    | 10.013544 | 268.63   |
| 500,100 | 30   | 0.0000    | 9.076082  | 421.46   |
从结果可以看出：
- 样本规模越大，执行时间越久
- 在本实验结果中，小样本和大样本的稀疏性都比中等样本规模好

### 3.4 Newton法

注：改变样本，没有体现太大规律，后续将不再改变该超参数。

**（1）改变L1正则化参数**

固定：样本数量为300，样本特征为50，初始点为全1向量。
改变：L1、L2正则化参数。

| L1、L2正则化参数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ---------- | ---- | --------- | --------- | -------- |
| 1e-5,1e-5  | 3    | 0.0000    | 10.013216 | 257.31   |
| 1e-4,1e-4  | -    | -         | -         | -        |
| 1e-3,1e-3  | -    | -         | -         | -        |
从结果可以看出：
- 当L1和L2取到1e-5,1e-5时，牛顿法收敛。
- 在1e-5,1e-5和1e-3,1e-3时，牛顿法结果不再收敛。考虑原因可能是正则项权重太大，牛顿法可能因为目标函数形状的剧烈变化而发散，无法找到最优解。

**（2）改变初始点**
固定：样本数量为300，样本特征为50，L1、L2正则化参数为1e-5,1e-5。
改变：初始点。

| 初始点      | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| -------- | ---- | --------- | --------- | -------- |
| 全0向量     | 3    | 0.0000    | 10.013216 | 257.31   |
| 全1向量     | 3    | 0.0000    | 10.013216 | 170.18   |
| -1,1随机向量 | 3    | 0.0000    | 10.013202 | 227.87   |
从结果可以看出：
- 以上所选取的三个初始点，牛顿法均收敛，且迭代次数一样。
- 从本次结果中未看出初始点对牛顿法的收敛影响

### 3.5 坐标下降法

**（1）改变L1正则化参数**

固定：样本数量为300，样本特征为50。
改变：L1、L2正则化参数。

| L1、L2正则化参数 | 迭代次数 | 预测误差（MSE） | 稀疏性（L1范数） | 执行时间(ms) |
| ---------- | ---- | --------- | --------- | -------- |
| 1e-5,1e-5  | 163  | 0.0000    | 10.013269 | 26147.93 |
| 1e-4,1e-4  | 121  | 0.0000    | 10.006727 | 26562.94 |
| 1e-3,1e-3  | 56   | 0.0001    | 9.964987  | 30842.23 |
从结果可以看出：
- L1、L2正则化参数越大，坐标下降法的迭代次数越小
- 坐标下降法的迭代次数不多，但执行时间很长。原因在于坐标下降法的 $\beta_{i}$ 更新过于麻烦。

## 4.总结

本实验旨在通过五种典型的优化方法（拉格朗日对偶法、梯度下降法、最速下降法、牛顿法和坐标下降法）来解决稀疏子集选择问题。在本次实验中，用了五种优化方法来解决上述问题，并对每种方法的收敛性和结果，选取不同的超参数进行了测试和分析。
经过本次实验，对上述五种典型的优化方法，及其适用场景进行简单总结：
- **拉格朗日法**。在处理带约束的优化问题时表现好，尤其适用于需要同时考虑多个目标（如预测误差和稀疏性）的场景。本次实验中，由于L0范数的不可导性和非凸性，拉格朗日法在求解过程中可能较为复杂，且通常只能得到近似解，无法保证解的全为零。
- **梯度下降法**。梯度下降法需要选择合适的学习率，否则可能导致收敛速度慢或无法收敛。此外，对于非凸问题，梯度下降法可能陷入局部最优解。因此，当目标函数是光滑且连续的时候，梯度下降法表现良好。
- **最速下降法**。本次实验中，最速下降法在每一步迭代中选择最优步长，从而加速收敛过程。本次实验的最速下降法应该是梯度下降法的上位替代。
- **牛顿法**。牛顿法收敛速度在接近最优解的时候很快。然而，该方法需要计算二阶导数，计算量大，且可能不适用于大规模问题。对于非凸问题，牛顿法可能因为目标函数形状的剧烈变化而发散。此外，初始点的选择对牛顿法的收敛性有较大影响（但本次实验没测出来）。
- **坐标下降法**。其适用于高维优化问题。然而，坐标下降法每次迭代的更新较为麻烦，计算成本较高，执行时间较长（本次实验中计算时间最长的）。