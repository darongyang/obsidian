<a href="https://www.usenix.org/conference/fast24/presentation/cho"> FAST'24 | RFUSE: Modernizing Userspace Filesystem Framework through Scalable Kernel-Userspace Communication </a>

## Motivation
### （1）FUSE的性能问题
- FUSE性能问题和可拓展问题。
- 原因：上下文切换、请求复制、单队列分派请求的锁争用

### （2）现有方案的问题
- 依赖于FUSE单队列设计，优化有限
- 不实用，很难和现有FUSE的文件系统兼容

## Insight
- 可拓展的内核-用户空间通信通道：每个核心的环形缓冲区
- 受io_uring启发：对两个共享队列的I/O轮询来减少系统调用的次数。这种思想也在NVMe和NIC中广泛使用。
具体说来是：提交队列和完成队列是内核与用户空间共享的。用户空间提交I/O请求到提交队列，内核线程会轮询这个提交队列。内核完成I/O请求后会将请求提交到完成队列，用户线程会轮询这个完成队列。

## Design
- 设计目标
	- 可拓展
	- 高效请求传输
	- 兼容性

## Result
和内核文件系统相当的吞吐量，具备高拓展性
- 相对于FUSE的时延减少53%
- RFUSE吞吐量与EXT4相近，随机读远高于FUSE
- RFUSE随着应用程序线程数的增加，I/O吞吐呈上升趋势，最终逼近EXT4
- 在元数据操作上，RFUSE始终相对于FUSE和EXTFUSE表现出优越的可伸缩性和卓越的吞吐量


## QA

Q1：我在测试结果中看到，extFUSE作为其核心对比对象，在很多测试中甚至不如没有优化的FUSE。针对这个问题，我想问：（1）这是个什么原因，这个测试是否公平，对比对象的复现是否合理；（2）本质上只有自己和naive方案在比，这个对比是否有点单薄？

Q2：在motivation提到FUSE-passthrough的工作，完全绕过了FUSE用户态的守护进程，这个用FUSE的意义在哪？为什么不直接使用内核态文件系统？这是出自一个什么场景的考虑？

Q3：我概述我对论文insight的理解，通过共享数据结构的轮询来减少频繁的切换。正如作者所述，这种思想并非独创，在`io_uring`、`NVMe`和`NIC`都被广泛使用。请问你觉得作者独到的创新在哪？

A3：主要体现在是首次将该思想引入fuse中。（1）新场景发现：观察到fuse场景有一样的切换痛点；（2）思想的定制化：宏观的思想是一致的，但新的fuse场景很多特性是不一样的，需要对应设计；（3）效果显著：测试结果证明有很好的效果。

Q4：在测试结果中，顺序读写上各个文件系统的差异并不是很大，而随机读写的差异很大，请问这是什么原因？
