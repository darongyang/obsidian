# PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU

## Motivation

消费级显卡：低内存，低带宽

现有方法不work：

（1）量化：效果有限

（2）offload：CPU/GPU走PCIe带宽低（FlexGen，DejaVu-UM），CPU/GPU计算负载不均衡（llama.cpp），导致推理慢

（3）稀疏化激活：现有的不适用于内存受限的消费级显卡（DejaVu-UM）

Insight 1：LLM推理神经元激活的倾斜特性很普遍，不管是层内还是模型内，还是不同任务间

Insight 2：某些冷激活层，CPU计算快速，会比PCIe+GPU计算更快

\*本地部署：batch通常为1，并行受限

\* 神经元激活是可以提前几层预测的？

## Key Idea

利用神经元的局部/倾斜性，小而热的放GPU，大而冷的放CPU，保持两者相对隔离的运算

## Design and Solution

挑战：

*   挑战1：在线预测器会带来内存开销
*   挑战2：实现高效动态的神经元稀疏算子
*   挑战3：频率、通信和内存的多约束优化

架构：

*   （1）离线分析神经元间的热度，确定分配；
*   （2）在线调度，C/GPU协同运算

模块：

*   （1）自适应预测器：如何高效识别热的神经元（应对挑战1）

预测器非固定大小，适应不同的倾斜性/稀疏性

预测器本身也是一个小模型，不断迭代训练保持参数量最小

*   （2）神经元管理：拆解后如何定位 + GPU/CPU混合执行：实现高效联合计算

拆解：引入神经元表记录位置，将模型拆解为DAG的全局任务队列

联合：C/GPU从队列取任务，检查执行依赖，每一层进行一次同步，让GPU合并结果

*   （3）神经元感知算子：聚焦神经元稀疏计算（应对挑战2）

GPU上算子，CPU上算子

*   （4）神经元放置策略：考虑各种约束的最优放置策略（对应挑战3）

最大化GPU上的神经元影响值

考虑唯一性、通信、内存等约束条件

## Result

实验配置：高配，i9+4090+PCIe4；低配，i7+2080Ti+PCIe3

评

*   使用场景局限：消费级显卡跑大模型，必要性？
*   insight局限：所有的大模型都有神经元幂等定理，都组用ReLU？
