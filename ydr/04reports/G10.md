
---

## Part 1

---
- 老师同学好。==我是==杨大荣，目前是硕士一年级，在做存储方向的研究，指导老师是夏文老师。
- 我今天==要分享的是==来自伊利诺伊大学香槟分校的团队在2023年发表在体系结构的四大顶会MICRO上的一篇工作，G10，通过智能张量迁移来实现高效统一的GPU内存和存储架构。
- 这次报告主要==关键词==是SSD存储、DNN训练还有，张量卸载。

---

- 我==将从==背景介绍、研究动机、设计实现、测试评估和论文总结五个方面进行论文的介绍。
（50s）

---

- 首先，是背景的介绍。DNN是深度神经网络的==简称==，目前已经被==广泛使用==于各种模型，包括CNN、Transformer等等。
- 因为我们主要对AI训练的系统做研究，所以我们从结构和流程来重新看一下DNN训练。从==结构上==看，模型主要包括输入/输出层，还有多个中间隐藏层。从==参数上==看，模型在训练中包含训练模型所需要的最基本的权重参数、还有许多中间过程的激活参数，它们都以张量形式存在。
- 一个宏观上的DNN==训练的示意过程==如图所示。主要包括参数的初始化、前向传播产生预测结果，计算损失函数，然后反向传播更新权重参数，不断重复到收敛。

---

- 然而随着模型的不断==膨胀==，现在的DNN训练对GPU的显存的需求急剧增加。
- 如这张图所示，GPU显存每两年才==翻==两倍，以transformer为例，而模型参数按照每两年翻410倍的速度增长。此外，训练过程中还需额外保留==中间==激活值，又要增加若干倍的内存占用。
- 从表和图中可以看到，现有的GPU==显存只有==24-80GB。在本文所选取的模型中，内存需求分别涵盖了150G到接近2T。有限的GPU显存难以容纳下训练过程中的所有参数，这个就是常说的内存墙问题。

---

- 现在GPU的内存和存储==体系==如图所示，==总体==包括显存、主存还有SSD存储。在==物理连接==上，SSD，GPU都各自通过PCIe接口和主机相连接。受限于PCIe接口的传输速率，GPU和主机数据传输的==最大带宽==为15GB/s。而受制于SSD自身的固有读写带宽，GPU和SSD的数据传输最大带宽为3GB/s。
- 针对显存受限的问题，主要的==思路==主要包括两个：使用主存来拓展显存、使用SSD来拓展显存。==英伟达==先后依次提出了统一内存UVM技术和存储直连GPU Direct Storage技术。==UVM技术==将主存和显存的统一管理和寻址，方便程序统一使用主机内存和GPU显存。==GPU Direct Storage技术==能够实现GPU和SSD直接数据传输，使得数据无需通过先从SSD传输到主机内存，再由主机内存传输到GPU的中转过程。
- 然而用主存来拓展显存依然会==面临==训练时内存不够用，且成本较高的问题，难以容纳更大的模型。而用成本相对更低的SSD拓展显存，能提供较大的容量，又面临访问速度受限的问题。

---

- 在==相关工作==上，发表在系统顶会ASPLOS 2023上的DeepUM和系统顶会FAST 2021上的FlashNeuron两篇工作分别针对这两种场景进行了优化。
- DeepUM通过围绕主存和显存两个结构设计，提出将显存放不下的部分数据卸载到主存，采用相关性预测器的方法，通过历史访问的数据模式来预测未来的数据访问模式来进行数据的预取。
- FlashNeuron则通过将部分的中间状态卸载到SSD，但权重参数仍全量保存在显存，依旧限制了可训练模型的大小。
- 但这两种方式都缺乏对张量全局访问模式的完整信息的感知。当预测不准确，导致使用到某权重，但是其不在显存中时，会发生缺页问题。
- 当发生缺页时，GPU的计算会被暂停，需要等待有关数据传输到显存，准备到位后才能继续进行计算。然而从SSD或主存传输的速度分别为3GB/s和15GB/s，相对于GPU的片上显存的1000GB/s访问速度要慢很多，带来计算的严重阻塞。

---

## Part 2

---

- 能否将主存和SSD同时用于拓展GPU的显存，同时尽可能避免上述的缺页问题，达到理想的无限大显存的GPU？G10进行了系列动机实验。

---

- 模型有很多层，分为很多==核函数==，这些核函数计算有前后的依赖。我们将被当前GPU正在执行的核函数所用到的张量称为==活跃==的张量。
- 如图所示，通过对实验所选取的模型进行分析，==横坐标==是模型的不同核函数索引编号，==纵坐标==是活跃张量所占模型峰值显存的比例。
- 可以发现==同一时刻==，也就是在同一核函数内，只使用很小一部分的张量。保持活跃的张量，大约占总显存需求的==10%==以内，平均在1%。所有核函数中最大的内存需求仅为5.7GB，远小于实验卡A100的40GB内存。
- 因此，G10的第一个==观察==是一个时刻内活跃张量==足够少==，相当部分的张量是用不到的，这也为张量交换提供了==足够多的==显存空间。

---

- 仅靠这一点并不够，我们将张量两次活跃之间的不活跃==间隔==定义为非活跃时间。
- 如图所示，==纵轴==代表张量的非活跃时间间隔大小，==横轴==代表区间的累计比例。作者进一步发现，对于==CNN==模型，60%张量的非活跃时间在$10^7us$，而对于==Transformer==模型，张量50%的非活跃时间在$10^5us$。
- 非活跃的时间==远远大于==SSD的访问延迟20us。这为张量的换入和换出提供了相当长的窗口期。这==意味==着卸载和预期张量的访问开销有机会被屏蔽。
- 这个特性是由DNN的训练过程对参数的稀疏访问所==决定==的。回顾在背景所介绍的DNN训练过程，同一个张量一般在前向传播和反向传播中各被访问一次，即使在更复杂的拓扑数据流下，也只是线性的访问次数。

---

- 除了上述两点观察，作者还发现模型中的张量的非活跃周期长度，以及张量的大小呈现很大的==差异性==
- 同时在运行时GPU的显存会发生动态的==变化==、GPU与SSD、GPU和CPU的传输带宽也会发生动态的变化，这为张量迁移策略的制定增加了很多复杂性。
- 不同的策略会带来不同优势和迁移的I/O成本。作者的核心见解是，最理想的情况下，应该是选择显存减少量最大、不活跃周期越长和I/O成本越低的张量。

---


## Part 3


基于上述观察和见解，作者提出了G10的设计。

---

- 首先，在==总体==架构上，G10主要包括三个大模块。
- 张量活跃分析器负责对DNN模型进行==编译和分析==，生成包含张量的依赖、大小、生命周期等语义信息。
- 张量迁移调度器接着按照分析得到的完整的张量信息，进行运行时的时间预估，在程序合适的地方对张量做==卸载和预取==。
- 最后G10修改GPU==硬件==驱动，实现了一套包含SSD、主存和显存的对用户透明的运行时统一内存，提供==统一==的寻址方式。

---

- 在张量活跃分析器的设计实现上，G10基于DNN训练过程中张量访问模式的==可预测性==，对模型进行编译和分析。
- 张量活跃分析器会分析所有张量的==生命周期==，自动得到张量创建和销毁的时机，及时分配和销毁显存。全局所需要的张量，如==W1==张量，G10会始终维护着，即使其不活跃；而仅在反向传播所需要的张量，如==dW1==张量，G10会运行时动态创建和及时销毁。
- 张量活跃分析器还会自动分析所有张量的==不活跃周期==，以便于将张量及时交换出GPU显存。当前核函数的输入输出都是活跃张量。一个张量的不活跃周期可能存在多个，可能意味着多次换入换出。G10的张量分析器会对所有情况进行维护。
- 张量活跃分析器是在==离线==阶段完成，其会进一步根据系统配置的传输带宽、算力情况对不活跃周期的具体时间和卸载/预取的具体时间进行一个==预估==。
- 最终，张量活跃分析器最后会生成整个系统决策所需要的完整的张量全局信息。

---

- 在张量迁移调度器的设计实现上，G10设计了动态且自动化的张量驱逐和预取策略。
- 在==驱逐==上，首先是==驱逐谁==，G10只驱逐当前不活跃张量，将迁移的==收益==考虑为综合张量大小和不活跃周期的乘积，即显存占用，如图中所示的阴影面积区域，并将迁移的==成本==考虑为迁移所需时间、对I/O带宽的占用成本。调度器通过==收益/成本比==动态选择当前显存中的最优张量进行迁移。

---

- 然后是==驱逐地点==的选择，主存带宽相对较大，但其容量小。当主存满了会发生从主存到SSD的二次数据交换，就是我们常说的换入换出。为了避免这种问题，G10采用的方法是分级的策略，==优先==将张量迁移到SSD，只有当SSD带宽拉满后，再将张量迁移到主存。
- 通过这种方式，G10能够==实现==GPU-SSD和GPU-HOST的带宽均衡，又能尽可能利用SSD的大容量。G10在迁移调度分析过程中会预估的内存压力状态和I/O流量状态，每次完成驱逐后都会对状态进行==更新==，以提供下一次驱逐决策使用。

---

- 在预取策略上，G10设置了==最晚==的安全预取时间，但如果都在这个时候预取，会导致带宽流量的阻塞和不均衡。
- G10观察到卸载张量后的GPU显存==空余较多==。其采取==积极==的预取策略，将最晚的安全预取时间排序，尽可能的提前安排预取。但要求提前预取所==借用==的显存占用，不会超过GPU显存上限而带来缺页中断；==否则==G10将保持原来的最晚的安全预取时间。这样的好处在于能够充分利用上所有时期的带宽。
- 通过上述驱逐策略和预取策略，G10能够自动的对模型程序进行==分析和预估==，得到完整的张量驱逐和预取策略，最后将这些策略以显式的==插桩==方式生成GPU程序。

---

- ==现有==的UVM技术仅支持显存和主存的统一寻址。基于上述的张量分析、迁移和调度策略。G10在底层==硬件==层面，进一步实现了加入SSD存储在内的拓展统一大内存UVM，来实现对上层透明的统一寻址。
- 在具体==实现==上，统一UVM会负责包括SSD页面在内的所有的地址==转换和分页==机制，并统一采用==4KB==页面管理。对于小的张量，G10会将其==合并==到一个页面中。在G10的UVM中，PTE页表项会==指向==显存、主存和SSD三个地方。SSD的==映射转换表==用于实现SSD的逻辑地址到物理地址转换，G10将其统一集成到UVM中完成统一的寻址转换。SSD的==垃圾回收==过程会迁移有效的页面，带来物理位置的变更，G10让垃圾回收过程直接对页表的PTE进行修改。此外，==张量迁移==也会带来物理位置的变更，也会对页表的PTE进行修改。预测错误时会产生==缺页中断==，其会严重阻碍计算，因此具有最高处理优先级。
- 因此，基于这些硬件设计，一个简单G10的预取/迁移的接口的==工作流程==如图所示。函数接口通过拓展UVM==定位==到张量物理位置，然后将待迁移的页面的==元数据==放入维护队列，多个迁移请求==打包==为一个传输集合一起迁移，充分利用传输带宽，然后按照迁移策略==分别==向SSD或主存迁移张量，完成迁移后，==修改==页表条目

---

- 在一些实现细节上，G10基于==Pytorch==做修改，然后在底层硬件修改上，基于UVMSmart和GPGPU-Sim进行==GPU的模拟==，复现从真实环境上收集的访问模式==Trace==，并使用SSDSim进行SSD的==模拟==。

---



Q1：gpu有专用的高速协议是nvlink，传输速度能够达到600gb/s。请问论文是否xxx？

Q2：论文是场景是单卡还是多卡？（回答单卡）现在训练都是多卡？既然如此的场景论文的适用吗？

Q3：为什么论文需要基于GPU仿真平台？而不是在真实的机器上直接运行？

Q4: 在线的优势来源于离线的分析，想请问作者是否有对离线的开销进行讨论。每次训练一个模型都进行一次离线分析，成本会不会较大。