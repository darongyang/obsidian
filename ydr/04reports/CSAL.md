## Introduction 7min
- 老师，同学们好，我是杨大荣，指导老师是夏文老师。今天我要分享的这篇论文是阿里云块存储团队和思得存储团队发表在计算机体系结构顶会EuroSys24上的一篇工作。CSAL，下一代云本地盘。本次报告的主要关键词是NAND闪存，缓存，和云存储。CSAL目前已经被广泛应用在云存储方案中。
- 我的汇报主要包括以下五个方面，背景，动机，设计，测试和总结。
- 首先是背景介绍。我先简单概述一下这篇论文，帮助大家能有个全局的了解。这篇论文基于这样一个背景。现在的云服务器通过虚拟化的方式为多个用户提供弹性云服务。然而有限的物理资源会限制单节点所能部署的虚拟机数量。近年来，计算资源按摩尔定律快速发展，带来了单个节点部署密度提升的可能。然而存储资源发展相对滞后，存储资源的容量和性能无法同时满足需求，成为云服务部署密度提升的瓶颈。基于此，本文则旨在通过结合一些新兴的设备，后续会进一步介绍，来提升云服务器的部署密度。
- 然后介绍一下云本地盘。在此之前，先介绍一下云服务。如刚才所说，云服务通过虚拟化物理资源的方式提供云服务，服务提供商和客户间会预先达成一定的服务水平目标，后续简称为SLO，即约定提供多算力，提供多少I/O性能，提供多少存储空间等等。云服务器的大小通常是固定的，一般是两个机架单位。在云服务中使用存储通常包括远程云盘和云本地盘，但远程云盘虽然能够灵活拓展存储容量，但其带宽较低；云本地盘相对来说更加适合高性能存储。本文聚焦云本地盘。云本地盘通常就是与服务器本地的固态硬盘SSD和机械硬盘HDD，挂载为块设备，通过SPDK/AIO的方式对其访问。Linux的LVM模块会通过虚拟化的方式将其虚拟化为多个逻辑设备。每个虚拟机使用其中若干设备。
- 在云服务中，通常会依据SLO来以一定的比例对物理资源粒度划分。固定的配置是为了是部署密度达到最大化，避免出现一些程序对资源的过分抢占导致部署密度降低。
- cpu的核心数和单核算力迅速发展，单服务器能够通过计算资源的复用，来实现多倍的部署密度提升。然而存储的资源的SLO则没有那么容易满足，它包括性能和容量两个方面。现在被广泛采用的机械硬盘带来大容量，但无法满足性能SLO。而HP-SSD，也就是高性能SSD带来很高的性能，但无法满足容量SLO。单纯的加盘不会提升性能，还会带来机架、成本等条件的制约。
- 近年来，新兴的存储设备QLC-SSD带来了和HDD相当的存储容量，且比HDD快10x的随机吞吐。QLC-SSD成为了解决存储SLO最理想的解决方案。现在市面上，QLC-SSD包括两种，第一种是通用QLC-SSD，它内部会有维护一个管理程序，叫做FTL。上层能够对这个设备进行一个任意方式的写操作。而另一种则叫做ZNS QLC-SSD，它内部不会维护这种管理程序，交给上层应用管理。它自己会格式化划分为若干的Zone，Zone内限制只能顺序写，上层自行完成随机写的管理。
## Motivation 6min-6.5min
- 作者将QLC-SSD进行了的若干尝试，结果发现现有方案事与愿违。第一次尝试是采用通用的QLC-SSD盘，将通用的QLC-SSD盘直接替代现在云存储方案中的HDD盘。上层通过LVM来实现通用QLC-SSD盘的复用。作者对该方案进行了测试，使用8个虚拟机，每个虚拟机内部使用FIO测试。
- 测试结果发现，在随机写上，通用的QLC-SSD只能和HDD性能持平，略有优势。在顺序写上，通用QLC-SSD的性能差距HDD落后很大，并且其性能和I/O单元有关。通用的QLC-SSD并不能满足写入性能上的SLO。
- 作者对其进行原因分析，发现是因为现在通用的QLC-SSD内部更多采用更大粒度的页面来减少管理程序的映射表的设备内部的内存占用。然而现有的操作系统的页面大多在4KB，作者对本地盘的I/O请求进行分析发现，小于等于8KB的小I/O占据了60%以上。对本地盘的小I/O请求会经过“读-修改-写”的过程，每次修改4KB的小页面，都会读取64KB的页面，并重新写入64KB的页面，带来写入量16x的放大 。作者把这个归纳为设备级的写放大。
- 第二个原因，作者认为是介质级写放大。QLC-SSD的擦除单元也会更大，不同生命周期的数据混杂在一起，会导致QLC-SSD在对失效的热数据进行垃圾回收的时候，大量的搬迁有效的冷数据，带来写入量的放大。作者把这个归纳为介质级写放大。因为这两种写入放大，为了满足99.9%用户2.54TB数据量写入的需求，QLC-SSD上实际进行了51.36TB数据量的写入。这大大超过了QLC-SSD能接受的上限，还严重影响其写入性能。
- 既然小I/O的影响如此之大，作者进行了第二个尝试，使用高性能缓存，即高性能SSD作为写回缓存，将I/O聚合成大I/O再一起写入。作者继续将通用型QLC-SSD作为主存储设备，并通过Open-CAS构建了该系统。
- 使用了相同的测试发现，在随机写上，Open-CAS性能有了明显的提升。在顺序写的小I/O上比通用型QLC-SSD有所提升，但在大I/O上逐渐落后。总体的性能落后于HDD。可见Open-CAS仍然无法满足写入性能的SLO。
- 作者对问题原因进行分析，虽然聚集带来小IO粒度上一定的性能优化，但其没有改变大粒度I/O的本质，还是会带来设备级的写放大，并且不命中情况的性能损失使其在大批量顺序写的写入性能不如通用型QLC-SSD。
- 既然前面基于通用QLC-SSD的方案都没法改变大粒度页面的困境，作者转向使用ZNS QLC-SSD方案。ZNS方案打破了设备内64KB页面的限制，但其Zone内只能支持顺序写。作者继续使用高性能SSD构建缓存来支持随机写。所有的随机写都路由到高性能SSD，顺序写则直接路由到ZNS QLC-SSD。作者通过dm-zoned构建了上述方案。
- 测试结果发现dm-zoned几乎没法用于随机写。在顺序写整体上比通用QLC-SSD的方案要好，但仍然大幅度落后于HDD。dm-zoned的方案无法满足写入性能上的SLO。
- 作者认为现有的zone方案过于低效，和zone进行一对一的映射，这带来频繁的填充，迁移和回收，对随机写的支持太差。
- 通过对现有方案的观察，作者认为主要原因是两点，从前两个实验得出现有的通用性QLC-SSD设备的方案因为两级写放大导致其写入性能不理想。而ZNS QLC-SSD带来了应对两种写放大的机遇。然而从dm-zoned实验得出，ZNS QLC-SSD带来了顺序写入性能的一定提升，但几乎没法支持随机写入。
## Design 约5min
- 基于这些观察，作者重新设计了一套块设备存储系统来解决这些问题。作者采用基于ZNS的多级存储架构。核心的设计包括，第一，采用两级L2P表支持随机写，通过细粒度页面管理应对设备写放大；第二，充分利用zns方案的特点，采用日志化的缓冲数据设计，按照生命周期对数据重新分类和合并，来应对介质级别的写放大问题。
- 具体设计上，首先是采用细粒度的页面管理，4KB页面来和操作系统对齐，来解决设备级的写放大。为了支持随机写，在Host端引入了L2P表。为了实现L2P表的内存卸载，减少内存占用，在保持4KB页面粒度的条件下，作者将其设计为了两级L2P表，类似于页表。最后顶级L2P表大小大致为4MB，而二级L2P表大小为16GB。
- 在存储架构上，作者将其划分为三层，第一层是DRAM，大概3GB，其专门用于缓存L2P表和全局元数据信息。L2P表只缓存L2P的所有顶级表项和部分二级表项，其余的卸载到下一层。
- 第二层是高性能SSD，大概730GB。高性能SSD会持久化存储所有的L2P表和全局元数据。此外，高性能SSD会存储和划分为若干下一级存储Zone的缓存，叫做Chunk。Chunk独特的设计在于：1.和Zone保持对齐，如大小，I/O等均和Zone保持相同，便于后续刷回；2.用作共享的数据缓冲区，用于后续分类整理数据。
- 第三层是ZNS QLC-SSD，大概15.36TB。该盘只用于存储从高性能SSD换出的数据，不会存储元数据。正如前面所说，划分为若干只能顺序写的Zone。Zone采用和Chunk相同的布局，用于接受从Chunk中迁移、分类和合并的数据。
- 在数据流操作上，读数据流比较简单，通过查询两级L2P得到对应的物理地址即可，然后从对应的HP-SSD和ZNS QLC-SSD读取。两级L2P表可能会带来读取的放大，但读取性能并不是SLO的瓶颈。
- 在写数据流上，对应三级存储架构，分别设计了三种写数据流。第一层，用户写。所有用户的所有写入都会写入到共享的高性能SSD。写入的时候会标记上对应的用户标签，以日志化的方式追加到Chunk的尾部。然后更新有关的L2P表项。
- 第二层，数据合并。当高性能SSD写满后，会按照之前的用户标签进行数据的分类和合并，每达到512KB进行一次迁移，将数据追加到该用户对应的Zone中。然后更新对应L2P表项。第三层，垃圾回收。在垃圾回收的设计上，作者将其和数据合并进行隔离，避免带来不同生命周期数据的混杂。然后更新有关的L2P表项。三个阶段的写流程都是并行的，会进行L2P表项的更新冲突。
- 在具体实现上，作者通过序列ID，SID来解决LBA冲突，服从最高的SID。当发生崩溃的时候，CSAL会通过P2L表的维护和检查点技术来加快L2P表的重构。在其他实现细节上，作者在SPDK上实现，为每个用户分配多个IO队列，并使用3个poller分别实现在线写，合并，和垃圾回收。
## Evaluation 约4min
- 在测试评估中，由于现有的QLC-SSD设备不好获取，作者通过两种方式分别来实现出16TB的ZNS QLC-SSD设备。第一种是使用通用QLC-SSD+SPDK来模拟出ZNS QLC。第二种则是通过4块4TB的ZNS TLC来模拟出ZNS QLC。作者采用了HDD，之前的3个方案和本论文的方案进行对比。
- 在裸设备的写性能上，可以看到，在均匀写入中，论文方案比第二好的opencas方案带来8.28x的性能提升，在顺序写上比dm-zoned带来5.24x的性能提升，接近达到HDD的水平。同样在倾斜写入上，本论文方案维持着显著的优势。综合两个结果来看，论文方案在小I/O上能达到最大的优化效果。
- 在读取性能方面，本论文方案和其他方案保持相同，均远超现有的HDD方案，并不是SLO的瓶颈。在读写混合上，CSAL也维持显著优势的读写带宽。
- 写入放大评估直接影响存储系统的性能，在写入放大评估上，在4KB的小I/O条件下， 采用ZNS的本论文方案相对于前两个基于通用QLC的方案具备很低的写入放大。当I/O粒度达到64KB后，每个方案的写放大的差距并不明显。
- 在每个功能的分解测试下，如GC和合并的效率，可以看到本论文方案具备最大的带宽，且不会阻塞用户在线写入。在隔离的效果上，隔离能够带来写入放大的优化。测试了2GB的L2P缓存相对于全部缓存的差距。对崩溃恢复优化的效果进行了评估，从全盘扫描的40分钟优化到5s以内。
- 作者在真实的数据库环境和生产环境中进行了该方案部署，均取得最优效果。
## Conlusion 约3min
- 下面对这篇论文做个简单总结。面对存储资源复用无法满足SLO，限制云服务部署密度的背景下，针对现在方案面临的两级写入放大和随机写失效的问题，本文提出了通过ZNS的多级存储架构来应对设备级写放大，并通过两级L2P表管理细粒度页面来支持随机写，通过对缓冲数据的日志化、分类和整理，来应对介质级写放大。
- 阅读本文，我也学习到了许多。在写作方面，attempts和lesson learned的写作方式，将纯粹的存储问题纳入宏观的云场景下，逻辑自洽。在工作方面，让我了解到通用QLC大页趋势及其面临的问题，ZNS设备其实是很灵活的，打破我对其固有的思维。
- 本文我认为其还存在一些不足，主要体现在一些关键挑战并不挑战，对应的设计点也相对简单。在不同方案对比下，成本的对比也是我比较关注的一个事情。还更多的一些discussion，如4KB通用QLC-SSD和存算分离等。本篇工作在我看来，有继续改进的空间，第一，其代码已经在SPDK开源，第二，是其数据分类方案效果有限，仍可进一步探索。
- 我今天的分享到此结束，请老师同学批评