
## 课程难点
- ==p6 16:01 证明：单纯型是多面体的一种==（难点在于：1.如何根据线性无关来构造矩阵B；2.消掉凸组合的θ0（对应后面的向量y放缩），3.结合B的秩（线性无关一定要用到的）进而构造A（列满秩->存在A能使得AB变成[Ik,0]），4.借助3对凸组合进行改造（同时乘以矩阵A，利用3，进一步拆解为若干等式），5.利用对y的放缩（舍弃θ0）和y性质来构造不等式）
- ==p7 36:14 证明：线性矩阵不等式的解集是凸集==（难点在于理解这个仿射函数及其构造：1.多元的矩阵变量变为一个矩阵；2.当值域取对称正定矩阵满足值域条件；3.而对称正定矩阵的集合是凸集，借助逆映射的保凸）
- ==p10 22:03 证明：凸函数的第三定义，即一阶条件==（难点在于：
（1）先证1维情况
注：1维情况下，一阶条件和第一定义都是一根筷子，都需要两个点完成，因此应该直接联想到用第一定义。确定两点x,y后：定义1构造凸组合；定义2构造差分。
- 1.构造指定的x'=x+t(y-x)（差分，联系定义3）<=>(1-t)x+ty(凸组合，联系定义1)，最后取极限lim t->0消掉t；
- 2.构造出第三个点z是x和y的凸组合（往定义1方向靠），用z，x来表示出f(x)和用z，y来表示出f(y)（借助定义3的差分不等式），再构造出函数值f(x)和f(y)的凸组合回归（往定义1方向靠），最后整理消掉z即可。
（2）再证高维情况
注：高维情况下，一定要降维，毫无疑问要联系到第二定义(一个高维的x和一个任意的一维的t和一个方向向量v)。第三定义是两个高维变量x，y的差分。
- 1.通过点y表示方向向量v，点x和点y来利用定义2，将高维降为1维关于t的凸函数g(t)，对g(t)求导得到导数g'(t)，利用上述1维情况已经证明的结论到t1，t2上，然后再取t1和t2的特殊值消掉t1 t2。
- 2.寻找最后证明g(t)是凸函数所需要的两点t1,t2，来构造两个同构的新点t1y+(1-t1)x和t2y+(1-t2)x，将这两点带入到f(x)的定义3中化简，化简完利用定义2得到g(t1)和g(t2)的式子进一步代换，最终得到g(t1)和g(t2)满足定义3（从t1 t2构成的新点f(xxxt1),f(xxxt2)+定义3出发，借助定义2的代换，最后回到了g(t1) g(t2)满足定义3）
- ==p13 06:38 证明：极大值函数的解析逼近log-sum-up函数是凸函数==（难点在于：1.表示其hessian矩阵，2.借助矩阵和向量运算的性质，简化hessian矩阵表示，3.利用二次型，让其显露出平方形式，4.进而利用柯西不等式判断正负）
- ==p13 39:00 证明：行列式的对数是凸函数==（难点在于：1.这个没法用二阶条件，高维向量得想到用定义2，构造z+tv；2.利用行列式的性质（乘积的行列式能变成行列式的乘积）和对称正定矩阵的变形技巧（能够展开成平方的形式）提出z和变出单位阵；3.利用对称正定矩阵的特征值分解的正交特性，仅留下特征值矩阵）；4.得到仅包含特征值的函数g(t)，对g(t)运用二阶条件结束。

- ==p14 38:50 证明：向量的top-k之和、实对称矩阵的最大特征值是凸函数== （前者难点在于：1.将每k个的x的组合看做Ax：关于向量x的仿射映射（乘上一个选择向量A），2.选择最大相当于在这些全部函数中取max，3.利用x是凸的，仿射和max保凸；后者难点在于：1.通过特征向量表示出特征值，2.这是关于原矩阵X的线性函数）
- ==p15 31:41 证明：函数组合/复合函数（高维情况）是的保凸结论== （h为凸，h拓展不降，g为凸，则f为凸）（难点在于：1.注意复合函数的定义域限制、以及凸集性质；2.利用好两个凸函数的第一定义；3.借助全空间不降迫使落在定义域）
- p20 08:20 （不太重要，了解一下就行）证明：可微拟凸函数的一阶条件（利用前两个定义）  （难点：
	- 构造微分的形式，建立一阶条件和定义2中max的同号形式，取θ->1，但这个稍微有点问题，只证明了θ->1；
	- 第二种方法是反证法，1.f(x)>f(y)满足一阶条件，构造z为x和y凸组合，要证f(x)>=f(z)（联系定义2）；2.假设f(x)<=f(z)，推出来必有f(x)=f(z)即可；3.f(y)<=f(z)，f(x)<=f(z)代入一阶条件，并利用z为x和y凸组合消掉第二项的z，最终得到f'(z)为0；4.由于函数连续，重复选择z'在z-x中重复，即可推出f(x)=f(z)
![[Pasted image 20241110163507.png]]
![[Pasted image 20241110163527.png]]
- ==p22 11:36 活动约束和不活动约束没理解==
- ==p23 27:24 证明：凸问题的局部最优就是全局最优== 难点在于：1.核心思路是反证法假设存在更优的y推出矛盾；2.利用这个假设点y构造凸组合z，利用凸函数的定义1得到f(z)、f(x)和f(y)关系（point 1）；3.取特殊的θ属于0-1，借助||y-x||2大于R，计算得到||z-x||2小于R，落在R内，得到f(y)<f(x)<f(z)关系（point 2）。此刻point 1和point 2产生矛盾。
![[Pasted image 20241110203946.png]]
- ==p26 04:58 证明：线性分数规划的等价变形== 难点在于：
	- =>根据线性分式函数，从x构造y和z
	- <=反过来，1. z≠0，根据上述的构造，由y和z构造x；2. z=0，代入简化新形式表示，只剩下y；3.任意选x0是P0可行解，构造x=x0+ty，x必对P0可行（代入P0 s.t.验证即可）；4.求f(x)函数值，最终和g(y)相同，取t于无穷（相当于已知y且z=0，由此构造一个新的x=x0+ty在P0中均满足可行解，当t取无穷时函数值还相同，证毕）
- ==p28 06:11 谱范数的等价变换== 难点在于：不理解为什么引入一个变量Y升维就能够使其满足条件了？为什么原来的不是凸优化问题（行列式下，约束行列式≥0，不是凸问题）？后面的就是凸优化问题了（引入新矩阵，是关于Y,x,t的线性式子，约束新矩阵的正定，是凸问题）？
- ==p28 36:10 多目标优化转化== 重点在于：1.多目标优化能够转为有约束的单目标；2.多目标可以利用pareto面的性质转为多个目标函数的加权平均；3.从而引出了拉格朗日乘子的概念：有约束的单目标 和 多个目标函数的加权平均 之间的关系（重点：目标和约束是可以相互转化的） （补充：在后续对偶问题中进一步说明，通过对偶问题求得的拉格朗日乘子，进一步计算得到的x就是多目标优化问题（转化为有约束的单目标）的最优解）

- ==p30 30:00 对偶问题的对偶问题== 重点在于：1.理解怎么求对偶问题，对偶函数求最大值变为对**负对偶函数求最小值**；2.理解原问题和对偶问题的对偶性（例如：目标维数和约束维数的相反性）；3.理解对偶问题一定是凸问题，类比函数的共轭一定是凸函数；4.理解给出例子中原问题的对偶问题的对偶问题的相似性（进一步思考什么时候（1）什么时候和自身相同；（2）什么时候$d*$和$p*$相同）
- ==p31 24:00 理解weak slater条件的正确性==。 重点在于：1.slater条件的D的相对内部，指的是“问题的域Domain”，而不是“问题的可行域Xf”。当对于weaker slater条件时，限制条件变成了可行域。2.分类讨论：（1）简单说就是，如果原问题f0的Ｄ是全空间，因此一定存在点ｘ满足线性约束条件（slater条件）；（2）如果原问题f0的Ｄ不是全空间，则其相对内部和线性约束如果有交集，即可行域，则一定存在点x满足线性约束条件（slater条件）
- ==对偶问题总结==：原问题->拉格朗日函数（引入拉格朗日算子）->对偶函数（对所有x属于D取最小值，得到只关于拉格朗日算子的函数）->对偶问题（对于对偶函数求最大值，要求第一种算子大于等于零。可转化为负对偶函数的最小值。最终得到的最优值$d*$是$p*$最接近下确界）->满足slater条件则两者相等 -> 进一步从四个角度解释了$d*$=$p*$（几何解释，鞍点解释，多目标优化解释，经济学解释）

- ==p35 29:38 $d*$=$p*$的鞍点解释== 难点在于：1.如何将原问题$p*$等价表示为inf sup的形式（利用sup如果不满足fi(x)≤0就会变为+∞的限制）2.有对偶问题的定义$d*$一定表示为sup inf的形式；3.如果存在 $d*$=$p*$，那么这些点就是这个拉格朗日函数的鞍点（鞍点解释）。
- ==p35 37:10 鞍点定理证明== 重点：
	- =>1.根据上述的鞍点解释，将$p*$等价表示为inf sup的形式（关于x的函数）和$d*$表示为sup inf的形式（关于朗姆达函数）；2.有鞍点的定义两个”点“+“值”相同，得到鞍点(x',朗姆达')的x'一定是inf sup最优解，而朗姆达'一定是sup inf最优解。
	- <=**1.** 最优解x'和朗姆达'一定各自满足约束不等式f(x')≤0和朗姆达'>0；**2.** 利用强对偶存在，$p*$=f0(x')=$d*$=g(朗姆达')；**3.** ==鞍点定义一半== 对g(朗姆达')=inf(x,朗姆达')放缩（只关于x函数，朗姆达已经得到最优解）。由于取inf(x)，g(朗姆达')一定小于等于代入x'的L(x', 朗姆达')小于等于f0(x')。利用2的强对偶存在，可以进行夹逼，把小于等于变为等于，得到g(朗姆达')，即inf(x,朗姆达')就是L(x', 朗姆达')（鞍点定理的一半）**4.** ==鞍点定义第二半== 直接分析sup(朗姆达,x')的值，利用1的不等式很容易得到最大值就是f0(x')，直接和3的夹逼连等为sup(朗姆达,x')就是L(x', 朗姆达')。==总结==：鞍点定义，对于inf一半，从$d*$定义出发，利用x'放缩，形成等式条件夹逼，另一半直接分析sup，回归上述等式
- ==**p36 17:17 KKT条件（全书最核心内容）**== 重点：
	- =>证明：从上述<=思路 **很容易** 得到强对偶下的四个KKT条件：原问题的可行性，对偶问题的可行性，互补松弛条件，稳定性条件（=>条件）。
	- 如果是==凸 可微 强对偶，则KTT条件是充要条件==。如果凸 可微，那么满足KKT，求解得到的对偶问题最优解就是原问题最优解，且对偶间隙为0！！
	- <=证明：1.只需要证明g(朗姆达',v')=f0(x')即可，然后用鞍点定理显然；2.利用凸问题限制，得到L(x,朗姆达',v')（注：只关于x的函数，固定了朗姆达',v'）也为凸函数，进而满足KKT的稳定性条件的x'就是全局最优；3.由g(朗姆达',v')定义，inf x L(x,朗姆达',v')，进而带入x'，进一步由KKT的互补松弛条件，最终得到g(朗姆达',v')=f0(x')。
- ==p38 06:00 区分互补条件和互补松弛条件，以及互补条件和KKT条件的联系==
- ==p40 02:00 证明：敏感性3个性质==。
	- 性质1：p*(u,w)是凸函数。难点：1.准确表示p*(u,w)；2.将去掉inf的部分构造为关于x,u,w的新目标函数g，很容易得到目标函数和定义域都是凸集（借助约束函数是凸函数+α-sublevel-set），进而g是x,u,w的联合凸；3.再考虑加上inf x的凸性，对g左右两边用凸函数的定义，然后同时取inf x1,x2，等价变形为p*(u,w)仍满足凸函数定义。
	- 性质2：凸 强对偶的不等式。难点：1.从p*(0,0)定义出发，将拉格朗日函数的约束条件fi(x')和hi(x')全部用干扰问题的不等式放缩为ui,wi
- ==p40 37:00 理解拉格朗日松弛和LP松弛的关系== 重点在：boolen LP问题的等价问题的对偶问题，又称为拉格朗日松弛，是有损失的下届求解。boolen LP问题的LP松弛的对偶问题，就是拉格朗日松弛。
- ==p41 28:00 从对偶来分析罚函数方法== （罚函数思想：罚函数可以不满足约束条件而运行，但，随着α不断增大，一定会迫使约束条件成立，不然没法取得min）从对偶函数来分析，罚函数对应对偶函数g(v)中某个特定取值的g(v')，其不一定满足 $d*$=$p*$，本质上是一个松弛。但是当α->∞时，要想使不等式成立，只能把和α相乘的项强制为0。这个时候罚函数就逼近原问题最优解了。

- ==p44 30:00 最优性条件证明== 分析梯度tf(x)趋近0时，x是否趋近x*，f(x)是否趋近f(x*)？想办法构造出|| f(x) - f(x*) ||和|| x - x* ||。难点在于：
	- 强凸函数也有一阶和二阶定义
	- || f(x) - f(x*) ||。从强凸函数一阶定义出发，1.固定x，变量y，求得右边式子的最小值，放缩。2.将强凸函数的y代换成x*，加上范数出现|| f(x) - f(x*) ||即可
	- || x - x* ||。从强凸函数一阶定义出发，1.将强凸函数的y代换成x*；2.将强凸函数右边用柯西不等式放缩出现||x-x*||；3.利用强凸函数的左边将f(x*)放缩为任意f(x)，进而消掉f(x)即可。
- ==优化算法预备知识小结==：优化的一般形式（迭代）->步长搜索方法->强凸的性质->最优性条件
- ==p45 证明问题是等价的 ==：1.最优解是一样的就是等价问题；2.通过变量代换将原来的一个问题进行等价变形得到另一个等价问题
- ==p46 03:00 梯度下降法收敛性证明== 核心是要找出 || f(xk+1) - p* || 和 || f(xk) - p* || 的关系，构造的时候要往这个方面去靠 
	- 精确搜索（α最优）：1.有$d^k$是xk的负梯度；2.将xk+1和xk代入强凸函数定义，右边看做α函数，代入1的条件化简；3.由于精确搜索，可以对上述有关α的函数求最小值；4.等式两边同时减去$p^*$构造上述关系
	- amijo搜索（0≤α≤1/M）：1.证明当0≤α≤1/M时，利用不等式放缩处理如上的2，得出amijo规则一定满足停止迭代条件。2.讨论α最终值（初值或迭代停止值），将其将代入上述迭代停止条件，求最小值min{xxx,xxx}。3.同上面减去$p^*$构造。
- 最速下降法，本质是要找每一步下降最陡的v，但是要对v进行规范化限制（例如||v||1=1或||v||2=1）。当对v取2范数限制的时候，本质v就是负梯度方向，就是梯度下降法。而当v取1范数限制的时候，本质v是和某个坐标轴平行的方向，取决于梯度哪个方向的绝对值最大，简单理解皆为梯度分量最大的反方向，这个就是最速下降法。进一步还可以选择无穷范数。
- ==p49 32:30 有约束+非线性方程组==。无解析解，用迭代思想。
	- 牛顿法：1.将问题等价表示为迭代的形式xk+d，2.利用泰勒二阶展开为关于d的线性方程组；3.利用迭代性质，得到关于d的新约束；4.得到新关于d优化问题的KKT条件，算出最优方向dk，进而算出最优步长α，进而算出xk+1。
	- 拉格朗日法：**（1）鞍点解释**。1.从拉格朗日函数出发，假设$v*$已知，求解$x*$。2.找到上述式子关于x的负梯度方向，代回迭代表达式中。3.然而，每一步中，$v*$是未知的，代入近似解$vk$。上述步骤反之对v亦然，区别在于v取梯度方向。**（2）罚函数解释**。1.构造KKT条件的误差罚函数，是关于x和v的。2.分别对x和v求偏导，得到该函数的负梯度方向。2.求负梯度方向和拉格朗日函数方向的内积，是大于0的，说明是下降方向。
	- 增广拉格朗日法：原来的拉格朗日迭代太慢了，而且很受限于步长α。。。为此：1.加入一个惩罚项，类比一次项，引入常数c。2.将xk到xk+1的梯度迭代，修改为固定vk直接求出最优的x作为xk+1（加速迭代，弄掉α），3.修改vk更新，使用xk+1（代换掉α，用惩罚项的常数c替代）
## 课程总结
### 一、凸集
![[1730474257878.png]]
![[Pasted image 20241101232039.png]]
### 二、凸函数
![[Pasted image 20241101232107.png]]
![[Pasted image 20241101231958.png]]
### 三、凸问题
![[Pasted image 20241101232208.png]]

### 四、对偶
![[Pasted image 20241101232235.png]]
![[Pasted image 20241101232327.png]]
![[Pasted image 20241101232352.png]]
### 五、优化算法
![[Pasted image 20241101232437.png]]

p1
- 数学优化的概念与定义 5:30
- 典型的凸优化问题：数据拟合问题，线性二次调节器，多用户的能量控制问题，图像处理问题，超大规模集成电路，==最短路径==
- 一些规划的分类
	- 线性规划与非线性规划
	- 凸规划与非凸规划
	- 光滑规划与非光滑规划
	- 连续规划与离散规划
	- 单目标规划与多目标规划
- 课程主要内容
	- 凸集 凸函数 凸优化
	- 凸优化
	- 若干算法
- ==优化发展的历史==

P2
- 直线与线段
- 仿射集、仿射组合、与仿射集相关的子空间
-  eg: 线性方程组的解集是一个仿射集






## 快速笔记

#### todo 向量二范数等价表示、矩阵向量求导
![[Pasted image 20241111102322.png]]

### 矩阵概念

#### 化零空间

矩阵的**化零空间**（null space），也称为**零空间**，是指一个矩阵 $A$ 的所有使 $A \mathbf{x} = 0$ 成立的向量 $\mathbf{x}$ 的集合。换句话说，化零空间是所有被矩阵 $A$ 映射到零向量的向量组成的空间。
如果 $A$ 是一个 $m \times n$ 的矩阵，那么化零空间是定义在 $\mathbb{R}^n$ 中的向量集合：$$
\text{化零空间} = \{ \mathbf{x} \in \mathbb{R}^n \mid A \mathbf{x} = 0 \}.
$$化零空间的性质
- **向量空间**：化零空间本身是一个向量空间，因为它包含零向量，并且对加法和标量乘法封闭。
- **维数**：化零空间的维数称为矩阵 $A$ 的**零空间维数**或**零度**（nullity），这是矩阵的一个重要性质。根据秩-零度定理，有：$$
   \text{rank}(A) + \text{nullity}(A) = n,
   $$其中 $n$ 是矩阵 $A$ 的列数。
- **解的空间**：化零空间可以用来描述线性方程组 $A \mathbf{x} = 0$ 的解的集合。非零解的存在与矩阵的列之间的线性相关性有关。如果化零空间仅包含零向量，那么矩阵的列是线性无关的。
化零空间可以看作是矩阵 $A$ 映射到零的所有向量的集合。这个空间的维数决定了矩阵的列之间的线性依赖关系。化零空间的维数越大，矩阵的列之间的线性相关性就越强。

#### ==正交矩阵==

**正交矩阵**是指一种特殊的方阵，它的列向量（或者行向量）是单位向量且彼此正交。具体来说，如果矩阵 $Q$ 满足以下条件：
$$
Q^T Q = Q Q^T = I
$$
其中 $Q^T$ 是 $Q$ 的转置矩阵，$I$ 是单位矩阵，那么矩阵 $Q$ 就是一个正交矩阵。
正交矩阵的性质：
- **向量的正交性**：正交矩阵的列向量彼此正交，并且每个列向量的长度为 1。同样，它的行向量也具有正交性并且是单位长度。
- **逆矩阵**：正交矩阵的转置矩阵 $Q^T$ 是它的逆矩阵，即 $Q^{-1} = Q^T$。这意味着$$
   Q Q^T = Q^T Q = I
   $$  
- **行列式**：正交矩阵的行列式的绝对值为 1，即
$$
   |\det(Q)| = 1
   $$
   因此，正交矩阵的行列式要么是 1，要么是 -1。
- **保距性**：正交矩阵保持向量的长度不变，即如果 $x$ 是一个向量，那么
$$
   \| Q x \| = \| x \|
   $$这意味着正交矩阵代表了一个“刚性变换”（例如旋转或反射），不会改变向量的长度或夹角。
- **特征值的模长**：正交矩阵的特征值的模长为 1。

#### 行列式
行列式的值可以揭示矩阵是否可逆、矩阵表示的线性变换的伸缩因子，以及向量组是否线性无关等信息。
（1）定义
对于一个 $n \times n$ 的方阵 $A$，其行列式记作 $\det(A)$ 或 $|A|$。行列式的值是一个实数或复数，取决于矩阵的元素。
在几何上，行列式的值可以解释为由矩阵列向量构成的“体积”：
- 在二维空间中，行列式表示由两个向量生成的平行四边形的面积。
- 在三维空间中，行列式表示由三个向量生成的平行六面体的体积。
- 对于更高维度，行列式可以看作是超体积的一种度量。
行列式的符号还表示了这种几何体是否经历了反转：如果行列式为正，几何体的方向不变；如果行列式为负，几何体发生了反转。
（2）性质
行列式有以下几个重要的性质：
行列式的计算有许多重要的性质，这些性质在矩阵运算和线性代数应用中非常有用。以下是行列式的主要性质：
- **交换行或列的性质**：如果交换矩阵的两行（或两列），行列式的值会变为原行列式的相反数。换句话说，交换一次行或列会使行列式的符号改变。
- **倍乘性质**：如果矩阵的一行（或一列）都乘以一个常数 $k$，那么行列式的值也会乘以 $k$。例如，如果矩阵 $A$ 的第 $i$ 行乘以 $k$ 得到矩阵 $B$，则 $\det(B) = k \cdot \det(A)$。
- **行列式为零的充要条件**： 如果矩阵 $A$ 的两行（或两列）相等或线性相关，则 $\det(A) = 0$。这意味着，若矩阵的行（或列）向量线性相关，则行列式为零。
- **可逆矩阵的行列式**：如果矩阵 $A$ 是可逆矩阵（即 $A^{-1}$ 存在），那么 $A$ 的行列式不为零，即 $\det(A) \neq 0$；反之，如果 $\det(A) = 0$，则 $A$ 是奇异矩阵，不可逆。
- **逆矩阵的行列式**：可逆矩阵 $A$ 的逆矩阵 $A^{-1}$ 的行列式等于 $A$ 的行列式的倒数，即 $\det(A^{-1}) = \frac{1}{\det(A)}$。
- **单位矩阵的行列式**：单位矩阵 $I$ 的行列式为 1，即 $\det(I) = 1$。单位矩阵不改变向量的方向或长度，所以行列式为 1。
- **对角矩阵的行列式**：对于对角矩阵（即除对角线外其他元素都为零的矩阵），其行列式等于对角线元素的乘积。例如，对角矩阵 $D = \operatorname{diag}(d_1, d_2, \dots, d_n)$，则 $\det(D) = d_1 d_2 \cdots d_n$。
- **三角矩阵的行列式**：上三角矩阵或下三角矩阵的行列式也是对角线上元素的乘积。
- **矩阵乘积的行列式**：两个矩阵 $A$ 和 $B$ 的乘积的行列式等于两个矩阵行列式的乘积，即 $\det(AB) = \det(A) \cdot \det(B)$。这条性质可以推广到多个矩阵的乘积。
- **转置矩阵的行列式**：矩阵 $A$ 的转置矩阵 $A^T$ 的行列式等于 $A$ 的行列式，即 $\det(A^T) = \det(A)$。换句话说，行列式与矩阵的行和列的排列无关。
- **行列加减性质**：对于矩阵 $A$，如果将一行（或一列）加上另一行（或一列）的倍数，行列式的值保持不变。这表示，行列式在进行初等行变换时保持不变（只要不进行交换）。
- **行列式的线性性质**： 如果矩阵的某一行（或一列）是几项之和，则行列式可以分解成对应几项之和的行列式。例如，如果 $A$ 的某一行是 $x + y$，则 $\det(A) = \det(A_x) + \det(A_y)$，其中 $A_x$ 和 $A_y$ 分别是用 $x$ 和 $y$ 替换该行得到的矩阵。
- **分块矩阵的行列式**：对于某些特殊形式的分块矩阵，可以用分块行列式公式计算。例如，如果矩阵 $A$ 是块对角矩阵，即$$
      A = \begin{bmatrix} A_1 & 0 \\ 0 & A_2 \end{bmatrix}
      $$那么 $\det(A) = \det(A_1) \cdot \det(A_2)$。

这些性质在矩阵运算和行列式计算中提供了许多便利。尤其是在求解线性方程组、特征值问题以及各种线性变换的应用中，行列式的这些性质非常有用。
（3）计算
对于小规模的矩阵（如 $2 \times 2$ 或 $3 \times 3$），可以直接计算行列式：
- 对于 $2 \times 2$ 矩阵 $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$，行列式为：
$$
  \det(A) = ad - bc
  $$
- 对于 $3 \times 3$ 矩阵 $A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}$，行列式为：
$$
  \det(A) = a(ei - fh) - b(di - fg) + c(dh - eg)
  $$
对于更高维度的矩阵，可以使用**展开法**（即按行或按列展开）或**LU 分解**等方法来计算行列式，但计算过程会更加复杂。

#### 非奇异矩阵

**非奇异矩阵**（Non-singular Matrix），也称为**可逆矩阵**（Invertible Matrix），是指一个方阵（即行数等于列数的矩阵）且其行列式不为零的矩阵。
（1）非奇异矩阵的定义
对于一个 $n \times n$ 的方阵 $A$，如果存在一个矩阵 $B$ 使得：
$$
A B = B A = I
$$
其中 $I$ 是 $n \times n$ 的单位矩阵，那么矩阵 $A$ 就是**非奇异的**或**可逆的**。在这种情况下，矩阵 $B$ 称为 $A$ 的逆矩阵，记作 $A^{-1}$。
（2）非奇异矩阵的特性
1. **行列式非零**：非奇异矩阵的行列式不等于零（即 $\det(A) \neq 0$）。这是判断一个方阵是否非奇异的充要条件。
2. **可逆性**：非奇异矩阵存在一个唯一的逆矩阵 $A^{-1}$。
3. **线性独立性**：非奇异矩阵的列向量（或行向量）是线性无关的，即矩阵的列空间或行空间可以张成整个 $n$-维空间。
4. **满秩**：对于一个 $n \times n$ 的非奇异矩阵，它的秩为 $n$（即满秩矩阵）。
（3）非奇异矩阵的应用
非奇异矩阵在线性代数中有许多重要应用：
- **解线性方程组**：对于方程 $A \mathbf{x} = \mathbf{b}$，如果 $A$ 是非奇异的，则方程有唯一解，可以通过 $\mathbf{x} = A^{-1} \mathbf{b}$ 计算。
- **矩阵分解**：非奇异矩阵在矩阵分解（如LU分解）中扮演重要角色。
- **线性变换**：非奇异矩阵对应的线性变换是双射，即每个输入有唯一的输出，且变换具有逆变换。
（4）奇异矩阵 vs 非奇异矩阵
- **奇异矩阵**：行列式为零，无法逆转，列向量或行向量线性相关。
- **非奇异矩阵**：行列式非零，有逆矩阵，列向量或行向量线性无关。

#### 正定矩阵
- **半正定**：一个矩阵 $A$ 是半正定的，如果对于任意的非零向量 $z$，都满足 $z^T A z \geq 0$。这意味着矩阵的特征值均为非负数。
- **严格正定**：一个矩阵是严格正定的，如果对于任意非零向量 $z$，都有 $z^T A z > 0$。这要求矩阵的特征值均为正数。

#### 对称矩阵

| 类型          | 定义                                     | 特性                      |
| ----------- | -------------------------------------- | ----------------------- |
| **对称矩阵**    | $A = A^T$                              | 特征值是实数，特征向量可以正交化        |
| **对称半正定矩阵** | 对称矩阵，且 $x^T A x \geq 0$ 对任意 $x \neq 0$ | 特征值非负，行列式非负             |
| **对称正定矩阵**  | 对称矩阵，且 $x^T A x > 0$ 对任意 $x \neq 0$    | 特征值为正，行列式正，具有Cholesky分解 |


#### 非负向量
- $\mathbb{R}_{++}^n$ 表示所有维度为 $n$ 的正实数向量空间的集合。具体来说，定义域 $\mathbb{R}_{++}^n$ 中的每一个元素 $x = (x_1, x_2, \dots, x_n)$ 满足 $x_i > 0$ 对于所有 $i$，也就是说，所有向量的每个分量都必须严格为正值。
- $\mathbb{R}_{+}^n$表示所有维度为 $n$ 的非负实数向量，即向量的分量可以是正数或零。而$\mathbb{R}_{++}^n$表示所有维度为 $n$ 的正实数向量，即所有分量必须严格大于零。

#### 三种范数
**范数**（Norm）是用于衡量向量或矩阵大小的一个函数。在不同的应用中，我们可以使用不同的范数来满足特定的需求。常见的范数包括 **0范数**、**1范数** 和 **2范数**。下面逐一介绍它们的定义、性质及常见应用。
**（1）0范数（$L_0$ 范数）**
**定义**：严格来说，0范数并不是一个真正的范数，因为它不满足范数的三角不等式，但在稀疏表示中被广泛使用。0范数通常定义为向量中**非零元素的个数**：

$$
\|x\|_0 = \text{the number of non-zero elements in } x
$$

**性质**：
- 0范数用于衡量向量的稀疏性（即向量中有多少个元素为非零）。
- 0范数不是连续的，因此在优化中通常会遇到困难。

**应用**：
- 0范数在稀疏信号处理、特征选择和压缩感知等领域中很常见。例如，Lasso回归中的稀疏约束就使用了与0范数类似的思想来选择有意义的特征。

**（2）1范数（$L_1$ 范数）**
**定义**：1范数是向量中所有元素绝对值的和，公式如下：

$$
\|x\|_1 = \sum_{i=1}^n |x_i|
$$

对于矩阵 $A$，1范数是矩阵的每一列绝对值和的最大值：

$$
\|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}|
$$

**性质**：
- 1范数是连续的，并且在优化中具有很好的性质，特别是对于稀疏优化问题。
- 1范数是凸的，这使得在凸优化问题中使用1范数约束或目标变得容易处理。

**应用**：
- 1范数在稀疏表示中也很常用，例如在Lasso回归中，用1范数代替0范数进行特征选择，因为1范数更容易优化。
- 1范数也用于信号处理中的噪声抑制，因为它可以帮助消除小噪声而保留显著特征。

**（3）2范数（$L_2$ 范数）**
**定义**：2范数，也称为欧几里得范数（Euclidean Norm），是向量中所有元素的平方和的平方根：

$$
\|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$

对于矩阵 $A$，2范数是最大奇异值：

$$
\|A\|_2 = \sigma_{\text{max}}(A)
$$

**性质**：
- 2范数表示向量到原点的欧几里得距离，是直观的距离度量。
- 2范数也是连续的，并且在许多应用中具有较好的平滑性。

**应用**：
- 2范数广泛用于机器学习和统计学中的正则化，例如Ridge回归中的 $L_2$ 正则化项可以防止过拟合。
- 在物理学和工程中，2范数是测量向量或信号能量的常用方法。

**(4) 总结**
- **0范数**：计数非零元素的个数，用于稀疏性度量，但不是一个真正的范数。
- **1范数**：元素绝对值的和，常用于稀疏优化问题，具有良好的凸性。
- **2范数**：元素平方和的平方根，表示欧几里得距离，常用于标准的距离度量和正则化。

#### 矩阵的秩
**等价定义**
- **线性无关行的最大数量**：矩阵的秩等于该矩阵中**线性无关行向量**的最大数目，称为**行秩**。
- **线性无关列的最大数量**：矩阵的秩等于该矩阵中**线性无关列向量**的最大数目，称为**列秩**。
- **最大非零子式的阶数**：矩阵的秩也等于该矩阵的非零子式的最大阶数。

如果秩小于矩阵的行数或列数，说明矩阵中的一些行或列是**线性相关**的，也就是说其中的某些行或列可以由其他行或列通过线性组合得到。

**计算矩阵的秩**
- **初等行变换**：通过高斯消元法将矩阵化为行阶梯形（Row Echelon Form）或简化行阶梯形（Reduced Row Echelon Form），非零行的数目即为矩阵的秩。
- **找最大非零子式**：通过寻找矩阵中非零的最大阶数的子矩阵（即子式），该子矩阵的阶数即为矩阵的秩。

#### ==特征值和特征值分解==
**（1）特征值（Eigenvalue）和特征向量（Eigenvector）**
对于一个 $n \times n$ 的方阵 $A$，如果存在一个非零向量 $\mathbf{v}$ 和一个标量 $\lambda$，使得

$$
A \mathbf{v} = \lambda \mathbf{v}
$$

那么我们称 $\lambda$ 是矩阵 $A$ 的一个**特征值**（eigenvalue），而对应的向量 $\mathbf{v}$ 则称为**特征向量**（eigenvector）。
- **特征值** $\lambda$ 表示矩阵作用在特征向量 $\mathbf{v}$ 上的“伸缩”程度：当 $A$ 作用在 $\mathbf{v}$ 上时，结果是 $\mathbf{v}$ 被缩放 $\lambda$ 倍，但方向不变。
- 特征值和特征向量反映了矩阵的“固有性质”。对于对称矩阵（或实对称矩阵），特征值都是实数，并且可以找到一组正交的特征向量。
- 特征值可以通过求解**特征方程**来获得，该方程为
$$
  \det(A - \lambda I) = 0
  $$
其中 $I$ 是单位矩阵，$\det$ 表示行列式。
**（2）特征值分解**
假设 $A$ 是一个 $n \times n$ 的方阵，如果存在一个可逆矩阵 $P$ 和一个对角矩阵 $D$，使得
$$
A = P D P^{-1}
$$
那么就称 $A$ 可以被特征值分解。
其中：
- $P$ 是由 $A$ 的特征向量构成的矩阵，每一列是 $A$ 的一个特征向量。
- $D$ 是一个对角矩阵，对角线上的元素是 $A$ 的特征值，通常记为 $\lambda_1, \lambda_2, \dots, \lambda_n$。
不是所有矩阵都可以进行特征值分解。特征值分解只适用于“对角化的矩阵”。一个矩阵 $A$ 可以对角化的充要条件是：
- $A$ 是一个方阵（必须是方阵才有特征值）。
- $A$ 有足够的线性无关的特征向量，即矩阵 $A$ 的特征向量可以构成一个基。
**特征值分解有以下重要性质：**
- **相似变换**：矩阵 $A$ 和 $D$ 通过相似变换（即 $A = P D P^{-1}$）相等，这意味着它们有相同的特征值。
- **对称矩阵的特征值分解**：对于一个实对称矩阵 $A$，特征值分解总是存在且特征向量可以正交化，即可以找到一个正交矩阵 $P$ 满足 $A = P D P^T$。
- **幂的性质**：通过特征值分解可以简化矩阵幂的计算。例如，如果 $A = P D P^{-1}$，那么 $A^k = P D^k P^{-1}$，其中 $D^k$ 是 $D$ 的每个对角线元素的 $k$ 次幂。
**（3）特征值和特征向量的求解步骤**
- **求解特征值**：解出矩阵 $A$ 的特征值。特征值 $\lambda$ 满足特征方程$$
   \det(A - \lambda I) = 0
  $$
   这里，$I$ 是单位矩阵，求解此方程得到 $\lambda_1, \lambda_2, \dots, \lambda_n$。
-  **求解特征向量**：对于每个特征值 $\lambda_i$，找到相应的特征向量 $v_i$，满足$$
   (A - \lambda_i I) v_i = 0
  $$
   通过解这个方程得到每个特征值对应的特征向量。（通常一个特征值会对应多个特征向量）
-  **构造矩阵 $P$ 和 $D$**：
   - 将特征向量 $v_1, v_2, \dots, v_n$ 按列组成矩阵 $P$。
   - 将特征值 $\lambda_1, \lambda_2, \dots, \lambda_n$ 填入对角矩阵 $D$ 的对角线元素上。

最终，可以得到 $A = P D P^{-1}$ 的形式。
#### 奇异值和奇异值分解

奇异值（Singular Value）和奇异值分解（Singular Value Decomposition, SVD）
对于任意一个 $m \times n$ 的矩阵 $A$，可以进行**奇异值分解**，即将矩阵 $A$ 分解为以下形式：
$$
A = U \Sigma V^T
$$
- $U$ 是一个 $m \times m$ 的正交矩阵，称为左奇异向量矩阵；
- $V$ 是一个 $n \times n$ 的正交矩阵，称为右奇异向量矩阵；
- $\Sigma$ 是一个 $m \times n$ 的对角矩阵，对角线上的元素称为**奇异值**（singular values），这些值是非负的，通常按从大到小排列。
奇异值是矩阵 $A$ 的特征值的广义定义，特别是当 $A$ 是非方阵时，仍然可以定义奇异值。
- 奇异值表示矩阵 $A$ 的“尺度变化”程度。奇异值较大的方向表明矩阵在该方向上伸缩较多。
- 奇异值是非负实数，并且可以看作是矩阵 $A^T A$ 或 $A A^T$ 的**平方根的特征值**。
- 奇异值分解能够将矩阵分解为三个部分，分别对应矩阵在行空间和列空间的结构信息。

（3）特征值与奇异值的区别与联系
- **矩阵类型**：特征值一般用于方阵，而奇异值适用于任意矩阵（包括非方阵）。
- **值的类型**：特征值可以是正数、负数或者复数，而奇异值始终是非负的实数。
- **应用场景**：特征值在描述线性变换的固有特性时尤为重要，而奇异值更多用于数据的降维和矩阵的秩分析。
（4）总结
- **特征值**是方阵固有的“伸缩因子”，对应的特征向量表示矩阵作用的特定方向。
- **奇异值**是任意矩阵的“尺度变化因子”，通过奇异值分解可以揭示矩阵的行和列空间特性。

#### 闭函数

###  凸函数的第二定义
（1）**概述**。设 $f: \mathbb{R}^n \to \mathbb{R}$ 是定义在 $\mathbb{R}^n$ 上的一个函数，我们想判断它是否为凸函数。这里，凸函数的第二定义给出了一种从一维函数的角度来判断高维函数是否凸的方法。
（2）**定义**。根据定义，若 $f$ 的定义域 $\text{dom } f$ 是凸集，且满足以下条件，则 $f$ 为凸函数：
- 对任意点 $x \in \text{dom } f$，以及任意方向向量 $v \in \mathbb{R}^n$，函数 $g(t) = f(x + t v)$ 是关于 $t$ 的凸函数。
其中，函数 $g(t) = f(x + t v)$ 是将高维函数 $f(x)$ 投影到一维的方式。
   - 对于给定的点 $x$，选择一个方向 $v$，考虑 $x$ 沿着方向 $v$ 移动的情况。
   - $t$ 是一个标量，表示沿着方向 $v$ 移动的“步长”。
   - 因此，$g(t)$ 就是沿着方向 $v$ 从 $x$ 点出发的“截线”上的函数值。这个过程相当于将高维的凸性问题转化为在该方向上的一维凸性问题。
（3）**凸性判断**。若对于任意点 $x \in \text{dom } f$ 和任意方向 $v \in \mathbb{R}^n$，函数 $g(t) = f(x + t v)$ 都是凸函数，那么可以证明原函数 $f(x)$ 是凸函数。
（4）**第二定义优点**。使用这种方法可以通过一维的凸性来间接证明高维函数的凸性。
   - 高维空间中的凸性直接验证会比较困难，因为涉及所有可能的线段。
   - 通过这种“降维”的方式，我们只需在任意方向 $v$ 上判断一维凸性，可以大大简化判断过程。

### 凸函数证明
如果 $f(x)$是二次可微的，则可以通过 Hessian 矩阵（即二阶导数矩阵）来证明凸性：
- 函数 $f(x)$是凸的，当且仅当$f(x)$的 Hessian 矩阵$H(f(x)) = \nabla^2 f(x)$ 是**半正定**，即对于任意的向量$z$，满足：   $$   z^T \nabla^2 f(x) z \geq 0   $$
- 如果 Hessian 矩阵是**严格正定**的，那么函数是**严格凸函数**。

### Hessian矩阵
Hessian矩阵（Hessian matrix）是一个方阵，用来描述一个多元函数的二阶偏导数。它反映了函数在每个变量上的曲率变化情况。

具体来说，若 $f(x_1, x_2, \dots, x_n)$ 是一个关于 $n$ 个变量的函数，Hessian矩阵 $H(f(x))$ 是包含所有二阶偏导数的矩阵，形式为：
$$
H(f(x)) = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \dots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

Hessian矩阵不仅能帮助我们分析函数的凸性或凹性，也能通过它的特征值来判定曲率变化。当 Hessian 矩阵是半正定的，则函数是凸的；如果是严格正定的，则函数是严格凸的。


### 柯西不等式
柯西不等式（Cauchy-Schwarz inequality）是线性代数和数学分析中的一个重要不等式，它描述了向量的内积与它们的范数之间的关系。
对于两个向量 $\mathbf{a} = (a_1, a_2, \dots, a_n)$ 和 $\mathbf{b} = (b_1, b_2, \dots, b_n)$，柯西不等式表示为：
$$
|\mathbf{a} \cdot \mathbf{b}| \leq \|\mathbf{a}\|_2 \|\mathbf{b}\|_2
$$

即向量内积的绝对值小于或等于各自二范数的乘积。它在证明和分析问题时非常常用，比如在凸优化、几何等领域。
柯西不等式的推论：**均值不等式**。对于两个非负数列的均值，算术均值大于或等于几何均值。$$\frac{a_1 + a_2 + \dots + a_n}{n} \geq \sqrt[n]{a_1 \cdot a_2 \cdot \dots \cdot a_n}$$


### 线性规划

#### 线性规划模型
**一、一般型**
目标函数：
$$ \text{max(min)} Z = \sum_{j=1}^{n} c_j x_j $$ 
约束条件：
$$ \sum_{j=1}^{n} a_{ij} x_j \leq (\text{或} =, \geq) b_i, \quad i = 1, 2, \dots, m $$ $$ x_j \geq 0, \quad j = 1, 2, \dots, n $$
**二、标准型**
（1）标准型要求
1. 目标函数为求最大值 
2. 约束条件均为等式方程 
3. 变量 $x_j$ 为非负
4. 常数$b_{i}$都大于等于0

（2）标准型形式
1.代数形式
$$ \text{max} \ Z = \sum_{j=1}^{n} c_j x_j $$ 受约束： $$ \sum_{j=1}^{n} a_{ij} x_j = b_i, \quad i = 1, 2, \dots, m $$ $$ x_j \geq 0, \quad j = 1, 2, \dots, n $$ 
2.矩阵形式
$$ \text{max} \ Z = C X $$ 受约束： $$ AX = b $$ $$ X \geq 0 $$

**三、转换方法**
- 目标函数为min问题：将目标函数取反获得max问题
- 约束条件不是等式：引入松弛变量/弹性变量转为等式
- 存在变量非负：将变量转为两个非负变量相减。即令$x_3=x_{3}^{'}-x_{3}^{''},x_{3}^{'}\ge{0},x_{3}^{''}\ge{0}$。
- 右端系数有负值：同时乘以-1

#### 单纯型法

**一、基本思想**
核心思想：通过在<u>可行域的顶点</u>之间移动，逐步逼近最优解。它利用几何特性，将线性规划问题的多变量约束条件描述为一个<u>凸多面体</u>，算法从该多面体的一个顶点开始，沿着边界在顶点之间移动，直至找到一个<u>能够最大化或最小化目标函数的顶点</u>，即最优解。

**二、基本概念**

![[Pasted image 20241011203804.png]]


![[Pasted image 20241011204043.png]]

**三、求解步骤**
![[Pasted image 20241011204344.png]]

![[Pasted image 20241012124229.png]]

![[Pasted image 20241012124831.png]]

![[Pasted image 20241012124727.png]]

![[Pasted image 20241012125704.png]]

![[Pasted image 20241012125840.png]]


#### 大M法

![[Pasted image 20241012163322.png]]


#### 两阶段法
以后学习
[【运筹学】-线性规划与单纯形法(三)(大M法与两阶段法：两阶段法)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1NL411c7dH/?spm_id_from=333.788.player.switch&vd_source=ab32fbac61c958a8816cc8ed84cb6438&p=2)


### 进化算法
智能进化算法（Evolutionary Algorithm, EA）是一类受到生物进化启发的优化算法，主要用于在大量可能解中找到最优或接近最优的解。这里我们介绍一些简单的进化算法，包括 **(1+1) EA on One Max** 和 **(1+1) EA on Leading One**，它们是进化算法中最基础的算法，非常适合入门者理解。
#### (1+1) EA on One Max
![[Pasted image 20241114203905.png]]
![[90397e7220462f2519e38eca36779401.png]]
#### (1+1) EA on Leading One
![[Pasted image 20241114203944.png]]
![[c9f3872e557df755783fc09c72f27747 1.png]]