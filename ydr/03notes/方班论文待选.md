- ==Baleen fast24==
问题：优化缓存的准入和预取策略，从而优化后端存储负载
动机：让缓存更智能
实现：引入“episodes”模型和磁盘头时间的优化目标，进一步做ML混入和ML预期
测试：优化在10-20%
==评价：比较一般，中规中矩，故事不好讲，动机不鲜明==

- ==Parrot osdi24==
问题：公共LLM服务的应用程序级信息缺乏（和Serverless LLM有什么不同？）
动机：应用程序级信息缺乏、依赖请求的额外开销、调度目标错位和冗余计算
实现：核心是引入语义变量，基于这个思路进行诸多优化
测试：
==评价：问题和动机明确，但是太偏大模型了，和存储有些偏颇==

- ==ServerlessLLM osdi24==
**问题：** 无服务计算下的大模型推理的冷启动延迟高、模型加载长、资源利用低下等。
**动机：** Serverless LLM的前景、现有系统面临上述问题
**实现：** 快速多级检查点、高效的推理实时迁移、检查点状态的调度
测试：
==评价：问题比较明确、但感觉动机有点少、设计实现比较新颖，有点考虑==

- ==G10 micro23==
问题：GPU内存拓展
动机：GPU内存墙、现有方案带宽受限、张量行为可以预测
实现：张量活性分析、智能张量迁移调度、统一的GPU内存架构
测试：达到ideal的90%
==评价：总体还行，动机明确，就是太偏GPU、内存和AI，和存储关系差有点多==
